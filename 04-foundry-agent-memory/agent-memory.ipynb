{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Foundry Agent Memory\n\n> **Author:** Ozgur Guler | AI Solution Leader, AI Innovation Hub\n> **Contact:** [ozgur.guler1@gmail.com](mailto:ozgur.guler1@gmail.com)\n> **© 2025 Ozgur Guler. All rights reserved.**\n\n---\n\nThis notebook demonstrates how to add persistent memory to agents in Azure AI Foundry.\n\n## What is Agent Memory?\n\nMemory in Foundry Agent Service is a managed, long-term memory solution that enables:\n- **Agent continuity** across sessions, devices, and workflows\n- **User preference retention** (e.g., \"I prefer dark roast coffee\")\n- **Personalized experiences** without repeating information\n\n## How Memory Works\n\nMemory is attached to agents using the **MemorySearchTool**:\n1. Create a **Memory Store** - container for memories with chat + embedding models\n2. Create a **MemorySearchTool** - tool that reads/writes to the memory store\n3. Attach the tool to a **Prompt Agent** - agent with memory capabilities\n4. Use **Conversations API** to interact with the agent\n\n## Prerequisites\n\nBefore running this notebook:\n\n1. **Azure CLI authenticated**: Run `az login` in your terminal\n2. **Azure AI Foundry project**: Created via Azure Portal or `azd`\n3. **Chat model deployment**: e.g., `gpt-4.1` or `gpt-5-nano`\n4. **Embedding model deployment**: e.g., `text-embedding-3-small` (notebook can deploy this)\n5. **Python packages**: `azure-ai-projects`, `azure-identity` (notebook installs these)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the preview SDK with memory support\n",
    "!pip install azure-ai-projects --pre --quiet\n",
    "!pip install azure-identity python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment from parent directory\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Configuration\n",
    "FOUNDRY_ACCOUNT = os.getenv(\"FOUNDRY_ACCOUNT_NAME\", \"ozgurguler-7212-resource\")\n",
    "PROJECT_NAME = os.getenv(\"FOUNDRY_PROJECT_NAME\", \"ozgurguler-7212\")\n",
    "PROJECT_ENDPOINT = f\"https://{FOUNDRY_ACCOUNT}.services.ai.azure.com/api/projects/{PROJECT_NAME}\"\n",
    "\n",
    "# Model deployments for memory\n",
    "CHAT_MODEL = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-5-nano\")\n",
    "EMBEDDING_MODEL = os.getenv(\"AZURE_TEXT_EMBEDDING_DEPLOYMENT_NAME\", \"text-embedding-3-small\")\n",
    "\n",
    "HOSTED_AGENT_NAME = \"my-hosted-agent\"\n",
    "MEMORY_STORE_NAME = \"hosted-agent-memory\"\n",
    "\n",
    "print(f\"Project Endpoint: {PROJECT_ENDPOINT}\")\n",
    "print(f\"Chat Model: {CHAT_MODEL}\")\n",
    "print(f\"Embedding Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Memory Store: {MEMORY_STORE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "# Initialize the client\n",
    "credential = DefaultAzureCredential()\n",
    "client = AIProjectClient(endpoint=PROJECT_ENDPOINT, credential=credential)\n",
    "\n",
    "print(\"AIProjectClient initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Section 1b: Verify Model Deployments\n\nMemory requires both a **chat model** and an **embedding model**. Let's verify they exist.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import subprocess\nimport json\n\n# Check existing model deployments\nprint(\"Checking model deployments...\")\n\nRESOURCE_GROUP = os.getenv(\"AZURE_RESOURCE_GROUP\", \"rg-ozgurguler-7212\")\nCOGNITIVE_ACCOUNT = FOUNDRY_ACCOUNT  # Same as AI Services account\n\nresult = subprocess.run(\n    [\"az\", \"cognitiveservices\", \"account\", \"deployment\", \"list\",\n     \"--name\", COGNITIVE_ACCOUNT,\n     \"--resource-group\", RESOURCE_GROUP,\n     \"-o\", \"json\"],\n    capture_output=True, text=True\n)\n\nif result.returncode == 0:\n    deployments = json.loads(result.stdout)\n    print(f\"Found {len(deployments)} deployments:\\n\")\n    \n    chat_found = False\n    embedding_found = False\n    \n    for d in deployments:\n        name = d.get(\"name\", \"\")\n        model = d.get(\"properties\", {}).get(\"model\", {}).get(\"name\", \"\")\n        print(f\"  - {name}: {model}\")\n        \n        if CHAT_MODEL in name or \"gpt\" in model.lower():\n            chat_found = True\n        if EMBEDDING_MODEL in name or \"embedding\" in model.lower():\n            embedding_found = True\n    \n    print(f\"\\n✅ Chat model ({CHAT_MODEL}): {'Found' if chat_found else 'NOT FOUND'}\")\n    print(f\"✅ Embedding model ({EMBEDDING_MODEL}): {'Found' if embedding_found else 'NOT FOUND'}\")\n    \n    if not embedding_found:\n        print(\"\\n⚠️  Embedding model not found! Run the next cell to deploy it.\")\nelse:\n    print(f\"Error checking deployments: {result.stderr}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Deploy embedding model if not found\n# This is REQUIRED for memory to work\n\nDEPLOY_EMBEDDING = False  # Set to True if embedding model is missing\n\nif DEPLOY_EMBEDDING:\n    print(f\"Deploying {EMBEDDING_MODEL}...\")\n    \n    result = subprocess.run([\n        \"az\", \"cognitiveservices\", \"account\", \"deployment\", \"create\",\n        \"--name\", COGNITIVE_ACCOUNT,\n        \"--resource-group\", RESOURCE_GROUP,\n        \"--deployment-name\", EMBEDDING_MODEL,\n        \"--model-name\", \"text-embedding-3-small\",\n        \"--model-version\", \"1\",\n        \"--model-format\", \"OpenAI\",\n        \"--capacity\", \"10\",\n        \"--sku\", \"GlobalStandard\"\n    ], capture_output=True, text=True, timeout=300)\n    \n    if result.returncode == 0:\n        print(f\"✅ Successfully deployed {EMBEDDING_MODEL}\")\n    else:\n        print(f\"❌ Error: {result.stderr}\")\nelse:\n    print(\"Embedding deployment skipped (DEPLOY_EMBEDDING = False)\")\n    print(\"Set DEPLOY_EMBEDDING = True if the embedding model is missing\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Create a Memory Store\n",
    "\n",
    "A memory store is the container for all memories. It defines:\n",
    "- Which models process memory (chat + embedding)\n",
    "- What types of memory to extract (user profile, chat summaries)\n",
    "- What information is relevant to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    MemoryStoreDefaultDefinition,\n",
    "    MemoryStoreDefaultOptions,\n",
    ")\n",
    "\n",
    "# Check if memory store already exists\n",
    "existing_stores = client.memory_stores.list()\n",
    "store_exists = any(s.name == MEMORY_STORE_NAME for s in existing_stores.data)\n",
    "\n",
    "if store_exists:\n",
    "    print(f\"Memory store '{MEMORY_STORE_NAME}' already exists\")\n",
    "    memory_store = client.memory_stores.get(MEMORY_STORE_NAME)\n",
    "else:\n",
    "    # Configure memory options\n",
    "    options = MemoryStoreDefaultOptions(\n",
    "        chat_summary_enabled=True,  # Store summaries of conversations\n",
    "        user_profile_enabled=True,  # Store user preferences/info\n",
    "        user_profile_details=\"Store user preferences, interests, and relevant context. Avoid sensitive personal data.\"\n",
    "    )\n",
    "\n",
    "    # Create the memory store\n",
    "    definition = MemoryStoreDefaultDefinition(\n",
    "        chat_model=CHAT_MODEL,\n",
    "        embedding_model=EMBEDDING_MODEL,\n",
    "        options=options\n",
    "    )\n",
    "\n",
    "    memory_store = client.memory_stores.create(\n",
    "        name=MEMORY_STORE_NAME,\n",
    "        definition=definition,\n",
    "        description=\"Memory store for hosted chat agent\"\n",
    "    )\n",
    "    print(f\"Created memory store: {memory_store.name}\")\n",
    "\n",
    "print(f\"\\nMemory Store Details:\")\n",
    "print(f\"  Name: {memory_store.name}\")\n",
    "print(f\"  Description: {memory_store.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Test Memory APIs Directly\n",
    "\n",
    "Before integrating with the hosted agent, let's test the memory APIs directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import ResponsesUserMessageItemParam\n",
    "\n",
    "# Define a test user scope\n",
    "TEST_SCOPE = \"test_user_001\"\n",
    "\n",
    "# Add some memories\n",
    "print(\"Adding memories...\")\n",
    "\n",
    "user_message = ResponsesUserMessageItemParam(\n",
    "    content=\"My name is Alex and I prefer concise technical answers. I work as a software engineer.\"\n",
    ")\n",
    "\n",
    "update_poller = client.memory_stores.begin_update_memories(\n",
    "    name=MEMORY_STORE_NAME,\n",
    "    scope=TEST_SCOPE,\n",
    "    items=[user_message],\n",
    "    update_delay=0  # Trigger immediately\n",
    ")\n",
    "\n",
    "# Wait for completion\n",
    "update_result = update_poller.result()\n",
    "print(f\"Memory update completed with {len(update_result.memory_operations)} operations\")\n",
    "\n",
    "for op in update_result.memory_operations:\n",
    "    print(f\"  - {op.kind}: {op.memory_item.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import MemorySearchOptions\n",
    "\n",
    "# Search for memories\n",
    "print(\"Searching memories...\")\n",
    "\n",
    "query = ResponsesUserMessageItemParam(content=\"What do you know about the user?\")\n",
    "\n",
    "search_result = client.memory_stores.search_memories(\n",
    "    name=MEMORY_STORE_NAME,\n",
    "    scope=TEST_SCOPE,\n",
    "    items=[query],\n",
    "    options=MemorySearchOptions(max_memories=5)\n",
    ")\n",
    "\n",
    "print(f\"Found {len(search_result.memories)} memories:\")\n",
    "for mem in search_result.memories:\n",
    "    print(f\"  - [{mem.memory_item.memory_id}] {mem.memory_item.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve static memories (user profile) without a query\n",
    "print(\"Retrieving static (profile) memories...\")\n",
    "\n",
    "static_result = client.memory_stores.search_memories(\n",
    "    name=MEMORY_STORE_NAME,\n",
    "    scope=TEST_SCOPE,\n",
    "    # No items = retrieve static/profile memories\n",
    ")\n",
    "\n",
    "print(f\"Found {len(static_result.memories)} profile memories:\")\n",
    "for mem in static_result.memories:\n",
    "    print(f\"  - {mem.memory_item.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 4: Create a Prompt Agent with Memory Tool\n\nInstead of modifying the hosted agent code directly, we attach memory to a **Prompt Agent** using the `MemorySearchTool`. This is the correct pattern for Foundry Agent Service.\n\nThe agent will:\n1. **At conversation start**: Automatically inject static memories\n2. **During conversation**: Retrieve contextual memories per turn\n3. **After response**: Update memories (debounced by update_delay)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from azure.ai.projects.models import (\n    MemorySearchTool,\n    PromptAgentDefinition,\n)\n\n# Define the user scope for memory partitioning\nUSER_SCOPE = \"demo_user_jordan\"\n\n# Create the memory search tool\nmemory_tool = MemorySearchTool(\n    memory_store_name=MEMORY_STORE_NAME,\n    scope=USER_SCOPE,\n    update_delay=1,  # Wait 1 second of inactivity before updating memories\n    # In production, use higher value like 300 (5 minutes)\n)\n\nprint(f\"Memory Tool configured:\")\nprint(f\"  Store: {MEMORY_STORE_NAME}\")\nprint(f\"  Scope: {USER_SCOPE}\")\nprint(f\"  Update Delay: 1 second\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a Prompt Agent with the memory search tool\nAGENT_WITH_MEMORY_NAME = \"chat-agent-with-memory\"\n\n# Check if agent already exists\ntry:\n    existing_agent = client.agents.retrieve(agent_name=AGENT_WITH_MEMORY_NAME)\n    print(f\"Agent '{AGENT_WITH_MEMORY_NAME}' already exists (version: {existing_agent.version})\")\n    agent = existing_agent\nexcept Exception:\n    # Create new agent with memory tool\n    agent = client.agents.create_version(\n        agent_name=AGENT_WITH_MEMORY_NAME,\n        definition=PromptAgentDefinition(\n            model=CHAT_MODEL,  # gpt-5-nano\n            instructions=\"\"\"You are a helpful AI assistant with memory capabilities.\nYou remember user preferences and past conversations.\nUse the memory tool to recall what you know about the user.\nBe friendly and personalize responses based on remembered information.\"\"\",\n            tools=[memory_tool],\n        )\n    )\n    print(f\"Created agent: {agent.name} (version: {agent.version})\")\n\nprint(f\"\\nAgent Details:\")\nprint(f\"  Name: {agent.name}\")\nprint(f\"  Version: {agent.version}\")\nprint(f\"  Model: {CHAT_MODEL}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get the OpenAI client for invoking the agent\nopenai_client = client.get_openai_client()\nprint(\"OpenAI client ready for agent invocation\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 5: Test Memory with Conversations API\n\nNow we'll test the agent with memory using the Conversations API. \n\nThe flow is:\n1. Create a conversation\n2. Send messages via `responses.create()` \n3. Memory is automatically updated after inactivity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Conversation 1: Introduce user preferences\nprint(\"=\" * 60)\nprint(\"Conversation 1: Introducing user preferences\")\nprint(\"=\" * 60)\n\n# Create a new conversation\nconversation1 = openai_client.conversations.create()\nprint(f\"Created conversation: {conversation1.id}\\n\")\n\n# Send a message with user preferences\nuser_input = \"Hi! My name is Jordan and I really love hiking and outdoor activities. I also enjoy photography.\"\n\nresponse = openai_client.responses.create(\n    input=user_input,\n    conversation=conversation1.id,\n    extra_body={\"agent\": {\"name\": agent.name, \"type\": \"agent_reference\"}},\n)\n\nprint(f\"User: {user_input}\")\nprint(f\"Agent: {response.output_text}\")\nprint(f\"\\nStatus: {response.status}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 6: Wait for Memory Extraction\n\nMemory updates are debounced by `update_delay`. We need to wait for inactivity before memories are stored."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\n\n# Wait for memory to be stored (update_delay=1 + processing time)\nprint(\"Waiting 65 seconds for memory extraction and storage...\")\nprint(\"(In production with update_delay=300, this happens in the background)\")\n\nfor i in range(65, 0, -5):\n    print(f\"  {i} seconds remaining...\")\n    time.sleep(5)\n\nprint(\"Done waiting!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Conversation 2: Test memory recall in a NEW conversation\nprint(\"=\" * 60)\nprint(\"Conversation 2: Testing memory recall (NEW conversation)\")\nprint(\"=\" * 60)\n\n# Create a completely new conversation\nconversation2 = openai_client.conversations.create()\nprint(f\"Created NEW conversation: {conversation2.id}\\n\")\n\n# Ask about what the agent remembers\nuser_input = \"What do you know about me?\"\n\nresponse = openai_client.responses.create(\n    input=user_input,\n    conversation=conversation2.id,\n    extra_body={\"agent\": {\"name\": agent.name, \"type\": \"agent_reference\"}},\n)\n\nprint(f\"User: {user_input}\")\nprint(f\"Agent: {response.output_text}\")\n\n# Check if agent recalled the user info\nresponse_lower = response.output_text.lower()\nif any(word in response_lower for word in ['jordan', 'hiking', 'outdoor', 'photography']):\n    print(\"\\n✅ SUCCESS: Agent remembered user information from previous conversation!\")\nelse:\n    print(\"\\n⚠️  Agent may not have recalled memories - check if memory was stored\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Conversation 3: Personalized recommendation\nprint(\"=\" * 60)\nprint(\"Conversation 3: Testing personalized recommendations\")\nprint(\"=\" * 60)\n\n# Continue in the same conversation\nuser_input = \"Can you suggest an activity for this weekend?\"\n\nresponse = openai_client.responses.create(\n    input=user_input,\n    conversation=conversation2.id,\n    extra_body={\"agent\": {\"name\": agent.name, \"type\": \"agent_reference\"}},\n)\n\nprint(f\"User: {user_input}\")\nprint(f\"Agent: {response.output_text}\")\n\n# If the agent suggests hiking/outdoor activities, memory is working!\nresponse_lower = response.output_text.lower()\nif any(word in response_lower for word in ['hik', 'outdoor', 'trail', 'nature', 'photo', 'camera']):\n    print(\"\\n✅ SUCCESS: Agent used memory to personalize recommendation!\")\nelse:\n    print(\"\\n⚠️  Response may not be personalized based on memories\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## Section 7: View Stored Memories\n\nLet's check what memories were extracted and stored."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# View all memories stored for our user scope\nfrom azure.ai.projects.models import MemorySearchOptions\n\nprint(f\"Searching memories for scope: {USER_SCOPE}\")\nprint(\"-\" * 60)\n\ntry:\n    # Search all memories for the scope\n    result = client.memory_stores.search_memories(\n        name=MEMORY_STORE_NAME,\n        scope=USER_SCOPE,\n        options=MemorySearchOptions(max_memories=10)\n    )\n    \n    if result.memories:\n        print(f\"Found {len(result.memories)} memories:\\n\")\n        for mem in result.memories:\n            print(f\"  ID: {mem.memory_item.memory_id}\")\n            print(f\"  Content: {mem.memory_item.content}\")\n            print()\n    else:\n        print(\"No memories found yet. Memory extraction may still be processing.\")\n        \nexcept Exception as e:\n    print(f\"Error searching memories: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Also check memories from our earlier API test\nprint(f\"Memories for test scope: {TEST_SCOPE}\")\nprint(\"-\" * 60)\n\ntry:\n    result = client.memory_stores.search_memories(\n        name=MEMORY_STORE_NAME,\n        scope=TEST_SCOPE,\n        options=MemorySearchOptions(max_memories=10)\n    )\n    \n    if result.memories:\n        print(f\"Found {len(result.memories)} memories:\\n\")\n        for mem in result.memories:\n            print(f\"  Content: {mem.memory_item.content}\")\n    else:\n        print(\"No memories found for test scope.\")\n        \nexcept Exception as e:\n    print(f\"Error: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 8: Cleanup (Optional)\n\nClean up resources when done testing."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Delete memories for a specific scope\nDELETE_SCOPE_MEMORIES = False  # Set to True to delete\n\nif DELETE_SCOPE_MEMORIES:\n    scopes_to_delete = [TEST_SCOPE, USER_SCOPE]\n    for scope in scopes_to_delete:\n        try:\n            client.memory_stores.delete_scope(\n                name=MEMORY_STORE_NAME,\n                scope=scope\n            )\n            print(f\"Deleted memories for scope: {scope}\")\n        except Exception as e:\n            print(f\"Error deleting scope {scope}: {e}\")\nelse:\n    print(\"Memory cleanup skipped (DELETE_SCOPE_MEMORIES = False)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Delete the agent created for memory demo\nDELETE_AGENT = False  # Set to True to delete\n\nif DELETE_AGENT:\n    try:\n        client.agents.delete(agent_name=AGENT_WITH_MEMORY_NAME)\n        print(f\"Deleted agent: {AGENT_WITH_MEMORY_NAME}\")\n    except Exception as e:\n        print(f\"Error deleting agent: {e}\")\nelse:\n    print(\"Agent deletion skipped (DELETE_AGENT = False)\")"
  },
  {
   "cell_type": "code",
   "source": "# Delete the entire memory store (use with caution!)\nDELETE_MEMORY_STORE = False  # Set to True to delete\n\nif DELETE_MEMORY_STORE:\n    try:\n        result = client.memory_stores.delete(MEMORY_STORE_NAME)\n        print(f\"Deleted memory store: {MEMORY_STORE_NAME}\")\n    except Exception as e:\n        print(f\"Error deleting memory store: {e}\")\nelse:\n    print(\"Memory store deletion skipped (DELETE_MEMORY_STORE = False)\")\n    print(\"⚠️  Warning: Deleting a memory store is irreversible!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### What We Built\n\n1. **Created a Memory Store** - Managed storage with chat + embedding models\n2. **Tested Memory APIs** - Add, search, and retrieve memories directly\n3. **Created a Prompt Agent with MemorySearchTool** - Agent that reads/writes memories\n4. **Verified Memory Persistence** - Agent recalls info across conversations\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| **Memory Store** | Container for memories with chat/embedding model |\n| **MemorySearchTool** | Tool that attaches memory to an agent |\n| **Scope** | Partition key for user-specific memories |\n| **User Profile** | Static info (name, preferences) |\n| **Chat Summary** | Distilled conversation topics |\n| **update_delay** | Wait time before extracting memories |\n\n### Correct Pattern for Memory\n\n```python\n# 1. Create memory store\nmemory_store = client.memory_stores.create(\n    name=\"my-memory\",\n    definition=MemoryStoreDefaultDefinition(...)\n)\n\n# 2. Create memory search tool\ntool = MemorySearchTool(\n    memory_store_name=memory_store.name,\n    scope=\"user_123\",\n    update_delay=60\n)\n\n# 3. Create agent with tool\nagent = client.agents.create_version(\n    agent_name=\"my-agent\",\n    definition=PromptAgentDefinition(\n        model=\"gpt-4.1\",\n        instructions=\"...\",\n        tools=[tool]\n    )\n)\n\n# 4. Use conversations API\nconversation = openai_client.conversations.create()\nresponse = openai_client.responses.create(\n    input=\"Hello!\",\n    conversation=conversation.id,\n    extra_body={\"agent\": {\"name\": agent.name, \"type\": \"agent_reference\"}}\n)\n```\n\n### Best Practices\n\n- Use unique scopes per user for privacy\n- Set `update_delay` to 5+ minutes in production\n- Use `scope={{$userId}}` for authenticated users\n- Only store necessary information\n\n### Pricing\n\nDuring preview, memory features are **free**. You only pay for chat/embedding model usage."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to `../05-observability-otel-to-appinsights` for agent monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n<div align=\"center\">\n\n## License & Attribution\n\nThis notebook is part of the **Azure AI Foundry Demo Repository**\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](../LICENSE)\n\n**Original Author:** Ozgur Guler | AI Solution Leader, AI Innovation Hub\n\n**Contact:** [ozgur.guler1@gmail.com](mailto:ozgur.guler1@gmail.com)\n\n---\n\n*If you use, modify, or distribute this work, you must provide appropriate credit to the original author as required by the [Apache License 2.0](../LICENSE).*\n\n**Copyright © 2025 Ozgur Guler. All rights reserved.**\n\n</div>",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}