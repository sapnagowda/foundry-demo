{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Observability: OpenTelemetry to Application Insights\n\n> **Author:** Ozgur Guler | AI Solution Leader, AI Innovation Hub\n> **Contact:** [ozgur.guler1@gmail.com](mailto:ozgur.guler1@gmail.com)\n> **© 2025 Ozgur Guler. All rights reserved.**\n\n---\n\nThis notebook demonstrates how to enable observability for Azure AI Foundry agents using OpenTelemetry and Application Insights.\n\n## What is Agent Observability?\n\nAgent observability provides:\n- **End-to-end tracing** of agent interactions\n- **Performance metrics** (latency, token usage, API calls)\n- **Debugging capabilities** for multi-agent systems\n- **Cost tracking** and optimization insights\n\n## How It Works\n\nAzure AI Foundry uses **OpenTelemetry** standards to capture traces:\n1. **Application Insights** stores the trace data\n2. **Foundry Portal** provides agent-specific views\n3. **Azure Monitor** enables full-stack observability\n\n## Prerequisites\n\nBefore running this notebook:\n\n1. **Azure CLI authenticated**: Run `az login`\n2. **Azure AI Foundry project**: From previous sections\n3. **Agent created**: From `../04-foundry-agent-memory`\n4. **Application Insights resource**: (notebook can create this)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install azure-ai-projects --pre --quiet\n",
    "!pip install azure-identity python-dotenv --quiet\n",
    "!pip install azure-monitor-opentelemetry opentelemetry-sdk --quiet\n",
    "!pip install opentelemetry-exporter-otlp --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment from parent directory\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Configuration\n",
    "FOUNDRY_ACCOUNT = os.getenv(\"FOUNDRY_ACCOUNT_NAME\", \"ozgurguler-7212-resource\")\n",
    "PROJECT_NAME = os.getenv(\"FOUNDRY_PROJECT_NAME\", \"ozgurguler-7212\")\n",
    "PROJECT_ENDPOINT = f\"https://{FOUNDRY_ACCOUNT}.services.ai.azure.com/api/projects/{PROJECT_NAME}\"\n",
    "RESOURCE_GROUP = os.getenv(\"AZURE_RESOURCE_GROUP\", \"rg-ozgurguler-7212\")\n",
    "\n",
    "# Model deployment\n",
    "CHAT_MODEL = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-5-nano\")\n",
    "\n",
    "# Agent from previous section\n",
    "AGENT_NAME = \"chat-agent-with-memory\"\n",
    "\n",
    "print(f\"Project Endpoint: {PROJECT_ENDPOINT}\")\n",
    "print(f\"Resource Group: {RESOURCE_GROUP}\")\n",
    "print(f\"Agent Name: {AGENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "# Initialize the client\n",
    "credential = DefaultAzureCredential()\n",
    "client = AIProjectClient(endpoint=PROJECT_ENDPOINT, credential=credential)\n",
    "\n",
    "print(\"AIProjectClient initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Connect Application Insights\n",
    "\n",
    "Application Insights stores all trace data. You can:\n",
    "1. **Create a new** Application Insights resource, or\n",
    "2. **Connect an existing** one to your Foundry project\n",
    "\n",
    "### Option A: Via Azure Portal (Recommended)\n",
    "\n",
    "1. Go to [Azure AI Foundry Portal](https://ai.azure.com)\n",
    "2. Select your project\n",
    "3. Navigate to **Observability** > **Tracing**\n",
    "4. Click **Connect Application Insights**\n",
    "5. Create new or select existing resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Application Insights connection string is available\n",
    "print(\"Checking Application Insights connection...\")\n",
    "\n",
    "try:\n",
    "    connection_string = client.telemetry.get_application_insights_connection_string()\n",
    "    print(f\"\\n✅ Application Insights connected!\")\n",
    "    print(f\"Connection string (truncated): {connection_string[:50]}...\")\n",
    "    \n",
    "    # Store for later use\n",
    "    APP_INSIGHTS_CONNECTION_STRING = connection_string\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️  Application Insights not connected: {e}\")\n",
    "    print(\"\\nTo connect Application Insights:\")\n",
    "    print(\"1. Go to https://ai.azure.com\")\n",
    "    print(\"2. Select your project\")\n",
    "    print(\"3. Go to Observability > Tracing\")\n",
    "    print(\"4. Connect an Application Insights resource\")\n",
    "    \n",
    "    APP_INSIGHTS_CONNECTION_STRING = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### Option B: Create via Azure CLI\n",
    "\n",
    "If you need to create Application Insights programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Only run if App Insights is not connected\n",
    "CREATE_APP_INSIGHTS = False  # Set to True to create\n",
    "APP_INSIGHTS_NAME = f\"{PROJECT_NAME}-appinsights\"\n",
    "\n",
    "if CREATE_APP_INSIGHTS and APP_INSIGHTS_CONNECTION_STRING is None:\n",
    "    print(f\"Creating Application Insights: {APP_INSIGHTS_NAME}...\")\n",
    "    \n",
    "    # Create Application Insights\n",
    "    result = subprocess.run([\n",
    "        \"az\", \"monitor\", \"app-insights\", \"component\", \"create\",\n",
    "        \"--app\", APP_INSIGHTS_NAME,\n",
    "        \"--location\", \"eastus\",\n",
    "        \"--resource-group\", RESOURCE_GROUP,\n",
    "        \"--kind\", \"web\",\n",
    "        \"--application-type\", \"web\",\n",
    "        \"-o\", \"json\"\n",
    "    ], capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        app_insights = json.loads(result.stdout)\n",
    "        APP_INSIGHTS_CONNECTION_STRING = app_insights.get(\"connectionString\")\n",
    "        print(f\"✅ Created Application Insights\")\n",
    "        print(f\"Connection String: {APP_INSIGHTS_CONNECTION_STRING[:50]}...\")\n",
    "        print(\"\\n⚠️  Note: You still need to connect this to your Foundry project via the portal.\")\n",
    "    else:\n",
    "        print(f\"❌ Error: {result.stderr}\")\n",
    "else:\n",
    "    if APP_INSIGHTS_CONNECTION_STRING:\n",
    "        print(\"Application Insights already connected\")\n",
    "    else:\n",
    "        print(\"Skipping App Insights creation (CREATE_APP_INSIGHTS = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Configure OpenTelemetry Tracing\n",
    "\n",
    "Now we'll set up OpenTelemetry to capture agent traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter\n",
    "\n",
    "# Enable content recording (captures input/output - disable in production if sensitive)\n",
    "os.environ[\"AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED\"] = \"true\"\n",
    "\n",
    "# Configure Azure Monitor if connection string available\n",
    "if APP_INSIGHTS_CONNECTION_STRING:\n",
    "    configure_azure_monitor(connection_string=APP_INSIGHTS_CONNECTION_STRING)\n",
    "    print(\"✅ Azure Monitor configured for Application Insights\")\n",
    "else:\n",
    "    # Fallback to console output for local testing\n",
    "    print(\"⚠️  No App Insights connection - using console exporter\")\n",
    "    tracer_provider = TracerProvider()\n",
    "    tracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n",
    "    trace.set_tracer_provider(tracer_provider)\n",
    "\n",
    "# Get tracer for our application\n",
    "tracer = trace.get_tracer(__name__)\n",
    "print(f\"Tracer initialized: {tracer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Test Tracing with Agent Invocation\n",
    "\n",
    "Let's invoke an agent and see the traces captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OpenAI client for agent invocation\n",
    "openai_client = client.get_openai_client()\n",
    "\n",
    "# Verify agent exists\n",
    "try:\n",
    "    agent = client.agents.retrieve(agent_name=AGENT_NAME)\n",
    "    print(f\"✅ Agent found: {agent.name} (version: {agent.version})\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Agent not found: {e}\")\n",
    "    print(\"Creating a simple test agent...\")\n",
    "    \n",
    "    from azure.ai.projects.models import PromptAgentDefinition\n",
    "    \n",
    "    AGENT_NAME = \"observability-test-agent\"\n",
    "    agent = client.agents.create_version(\n",
    "        agent_name=AGENT_NAME,\n",
    "        definition=PromptAgentDefinition(\n",
    "            model=CHAT_MODEL,\n",
    "            instructions=\"You are a helpful assistant for testing observability.\"\n",
    "        )\n",
    "    )\n",
    "    print(f\"✅ Created test agent: {agent.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Simple traced agent call\n",
    "print(\"=\" * 60)\n",
    "print(\"Test 1: Simple Traced Agent Call\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with tracer.start_as_current_span(\"agent-test-simple\") as span:\n",
    "    # Add custom attributes to the span\n",
    "    span.set_attribute(\"agent.name\", AGENT_NAME)\n",
    "    span.set_attribute(\"test.type\", \"simple\")\n",
    "    \n",
    "    # Create conversation and invoke agent\n",
    "    conversation = openai_client.conversations.create()\n",
    "    span.set_attribute(\"conversation.id\", conversation.id)\n",
    "    \n",
    "    response = openai_client.responses.create(\n",
    "        input=\"What is 2 + 2? Please respond briefly.\",\n",
    "        conversation=conversation.id,\n",
    "        extra_body={\"agent\": {\"name\": AGENT_NAME, \"type\": \"agent_reference\"}},\n",
    "    )\n",
    "    \n",
    "    span.set_attribute(\"response.status\", response.status)\n",
    "    \n",
    "    print(f\"\\nUser: What is 2 + 2?\")\n",
    "    print(f\"Agent: {response.output_text}\")\n",
    "    print(f\"\\nTrace ID: {span.get_span_context().trace_id:032x}\")\n",
    "    print(f\"Span ID: {span.get_span_context().span_id:016x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Multi-turn conversation with nested spans\n",
    "print(\"=\" * 60)\n",
    "print(\"Test 2: Multi-Turn Conversation with Nested Spans\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with tracer.start_as_current_span(\"agent-test-multiturn\") as parent_span:\n",
    "    parent_span.set_attribute(\"agent.name\", AGENT_NAME)\n",
    "    parent_span.set_attribute(\"test.type\", \"multi-turn\")\n",
    "    \n",
    "    # Create conversation\n",
    "    conversation = openai_client.conversations.create()\n",
    "    parent_span.set_attribute(\"conversation.id\", conversation.id)\n",
    "    \n",
    "    messages = [\n",
    "        \"Hello! I'm learning about AI agents.\",\n",
    "        \"What are some common use cases for AI agents?\",\n",
    "        \"Thanks! Can you summarize our conversation?\"\n",
    "    ]\n",
    "    \n",
    "    for i, user_input in enumerate(messages):\n",
    "        with tracer.start_as_current_span(f\"turn-{i+1}\") as turn_span:\n",
    "            turn_span.set_attribute(\"turn.number\", i + 1)\n",
    "            turn_span.set_attribute(\"user.input\", user_input[:100])\n",
    "            \n",
    "            response = openai_client.responses.create(\n",
    "                input=user_input,\n",
    "                conversation=conversation.id,\n",
    "                extra_body={\"agent\": {\"name\": AGENT_NAME, \"type\": \"agent_reference\"}},\n",
    "            )\n",
    "            \n",
    "            turn_span.set_attribute(\"response.status\", response.status)\n",
    "            \n",
    "            print(f\"\\nTurn {i+1}:\")\n",
    "            print(f\"  User: {user_input}\")\n",
    "            print(f\"  Agent: {response.output_text[:200]}...\" if len(response.output_text) > 200 else f\"  Agent: {response.output_text}\")\n",
    "    \n",
    "    print(f\"\\nParent Trace ID: {parent_span.get_span_context().trace_id:032x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Custom Tracing for Agent Tools\n",
    "\n",
    "You can add custom spans for any function or tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from opentelemetry.trace import SpanKind, Status, StatusCode\n",
    "\n",
    "def traced_tool_call(tool_name: str, input_data: dict) -> dict:\n",
    "    \"\"\"Example of a traced tool/function call.\"\"\"\n",
    "    with tracer.start_as_current_span(\n",
    "        f\"tool.{tool_name}\",\n",
    "        kind=SpanKind.INTERNAL\n",
    "    ) as span:\n",
    "        span.set_attribute(\"tool.name\", tool_name)\n",
    "        span.set_attribute(\"tool.input\", str(input_data))\n",
    "        \n",
    "        try:\n",
    "            # Simulate tool execution\n",
    "            start = time.time()\n",
    "            time.sleep(0.1)  # Simulate processing\n",
    "            result = {\"status\": \"success\", \"data\": f\"Processed {input_data}\"}\n",
    "            \n",
    "            span.set_attribute(\"tool.output\", str(result))\n",
    "            span.set_attribute(\"tool.duration_ms\", (time.time() - start) * 1000)\n",
    "            span.set_status(Status(StatusCode.OK))\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "            span.record_exception(e)\n",
    "            raise\n",
    "\n",
    "# Test custom tool tracing\n",
    "print(\"Testing custom tool tracing...\")\n",
    "\n",
    "with tracer.start_as_current_span(\"agent-with-tools\") as span:\n",
    "    # Simulate agent calling multiple tools\n",
    "    result1 = traced_tool_call(\"get_weather\", {\"city\": \"Seattle\"})\n",
    "    result2 = traced_tool_call(\"search_web\", {\"query\": \"AI agents\"})\n",
    "    \n",
    "    print(f\"Tool 1 result: {result1}\")\n",
    "    print(f\"Tool 2 result: {result2}\")\n",
    "    print(f\"\\nTrace ID: {span.get_span_context().trace_id:032x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Viewing Traces\n",
    "\n",
    "### In Azure AI Foundry Portal\n",
    "\n",
    "1. Go to [Azure AI Foundry](https://ai.azure.com)\n",
    "2. Select your project\n",
    "3. Navigate to **Observability** > **Tracing**\n",
    "4. Filter by time range, agent name, or trace ID\n",
    "\n",
    "### In Application Insights\n",
    "\n",
    "1. Go to [Azure Portal](https://portal.azure.com)\n",
    "2. Find your Application Insights resource\n",
    "3. Go to **Transaction search** or **End-to-end transaction details**\n",
    "4. Search by trace ID or time range\n",
    "\n",
    "### Agents View (Preview)\n",
    "\n",
    "Application Insights has a new **Agents View** specifically for AI agents:\n",
    "1. Open Application Insights\n",
    "2. Navigate to **Monitoring** > **Agents**\n",
    "3. View agent performance, errors, and usage patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate links to view traces\n",
    "print(\"=\" * 60)\n",
    "print(\"View Your Traces\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. Azure AI Foundry Portal:\")\n",
    "print(f\"   https://ai.azure.com/tracing?project={PROJECT_NAME}\")\n",
    "\n",
    "print(f\"\\n2. Application Insights (Azure Portal):\")\n",
    "print(f\"   https://portal.azure.com/#@/resource/subscriptions/*/resourceGroups/{RESOURCE_GROUP}/providers/microsoft.insights/components/*/overview\")\n",
    "\n",
    "print(f\"\\n3. Query traces with Kusto (in App Insights > Logs):\")\n",
    "print(f\"\"\"   \n",
    "   // Recent agent traces\n",
    "   traces\n",
    "   | where timestamp > ago(1h)\n",
    "   | where operation_Name contains \"agent\"\n",
    "   | project timestamp, operation_Name, message, customDimensions\n",
    "   | order by timestamp desc\n",
    "   | take 50\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Export Traces to OTLP Endpoint (Optional)\n",
    "\n",
    "You can also export traces to any OTLP-compatible backend (Jaeger, Grafana, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Configure OTLP exporter for third-party backends\n",
    "# Uncomment and configure for your OTLP endpoint\n",
    "\n",
    "ENABLE_OTLP = False  # Set to True to enable\n",
    "\n",
    "if ENABLE_OTLP:\n",
    "    from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n",
    "    from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "    \n",
    "    # Configure OTLP endpoint (e.g., Jaeger, Grafana Tempo)\n",
    "    otlp_endpoint = os.getenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\", \"http://localhost:4317\")\n",
    "    \n",
    "    otlp_exporter = OTLPSpanExporter(endpoint=otlp_endpoint)\n",
    "    trace.get_tracer_provider().add_span_processor(\n",
    "        BatchSpanProcessor(otlp_exporter)\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ OTLP exporter configured: {otlp_endpoint}\")\n",
    "else:\n",
    "    print(\"OTLP export skipped (ENABLE_OTLP = False)\")\n",
    "    print(\"\\nTo export to OTLP backends like Jaeger:\")\n",
    "    print(\"1. Set ENABLE_OTLP = True\")\n",
    "    print(\"2. Set OTEL_EXPORTER_OTLP_ENDPOINT environment variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practice: Use a decorator for consistent tracing\n",
    "from functools import wraps\n",
    "\n",
    "def traced(name: str = None, attributes: dict = None):\n",
    "    \"\"\"Decorator to add tracing to any function.\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            span_name = name or func.__name__\n",
    "            with tracer.start_as_current_span(span_name) as span:\n",
    "                # Add custom attributes\n",
    "                if attributes:\n",
    "                    for key, value in attributes.items():\n",
    "                        span.set_attribute(key, value)\n",
    "                \n",
    "                # Add function info\n",
    "                span.set_attribute(\"function.name\", func.__name__)\n",
    "                span.set_attribute(\"function.module\", func.__module__)\n",
    "                \n",
    "                try:\n",
    "                    result = func(*args, **kwargs)\n",
    "                    span.set_status(Status(StatusCode.OK))\n",
    "                    return result\n",
    "                except Exception as e:\n",
    "                    span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "                    span.record_exception(e)\n",
    "                    raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Example usage\n",
    "@traced(name=\"process_user_request\", attributes={\"service\": \"chat-agent\"})\n",
    "def process_request(user_input: str) -> str:\n",
    "    \"\"\"Process a user request with automatic tracing.\"\"\"\n",
    "    return f\"Processed: {user_input}\"\n",
    "\n",
    "# Test the decorator\n",
    "result = process_request(\"Hello, agent!\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush traces before exiting (important for batch exporters)\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "\n",
    "provider = trace.get_tracer_provider()\n",
    "if hasattr(provider, 'force_flush'):\n",
    "    provider.force_flush()\n",
    "    print(\"✅ Traces flushed to exporters\")\n",
    "else:\n",
    "    print(\"Tracer provider does not support force_flush\")\n",
    "\n",
    "print(\"\\nTraces should now be visible in:\")\n",
    "print(\"- Azure AI Foundry Portal (Tracing section)\")\n",
    "print(\"- Application Insights (Transaction search)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "1. **Connected Application Insights** to Azure AI Foundry project\n",
    "2. **Configured OpenTelemetry** for trace collection\n",
    "3. **Traced agent invocations** with custom spans\n",
    "4. **Added custom tool tracing** for internal operations\n",
    "5. **Created reusable tracing utilities** (decorator pattern)\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Span** | A single unit of work (e.g., agent call, tool invocation) |\n",
    "| **Trace** | Collection of spans forming an end-to-end request |\n",
    "| **Trace ID** | Unique identifier linking all spans in a trace |\n",
    "| **Attributes** | Key-value metadata attached to spans |\n",
    "| **Exporter** | Sends traces to a backend (App Insights, OTLP, Console) |\n",
    "\n",
    "### Environment Variables\n",
    "\n",
    "```bash\n",
    "# Required\n",
    "PROJECT_ENDPOINT=https://<account>.services.ai.azure.com/api/projects/<project>\n",
    "\n",
    "# Optional - auto-detected from Foundry\n",
    "APPLICATION_INSIGHTS_CONNECTION_STRING=InstrumentationKey=...\n",
    "\n",
    "# Optional - enable input/output recording (disable in production)\n",
    "AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED=true\n",
    "\n",
    "# Optional - OTLP export\n",
    "OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Name spans descriptively**: Use `agent.chat`, `tool.search`, etc.\n",
    "- **Add relevant attributes**: agent name, user ID, conversation ID\n",
    "- **Handle errors**: Use `span.record_exception()` for debugging\n",
    "- **Disable content recording in production**: Protects sensitive data\n",
    "- **Flush traces before exit**: Ensures all data is exported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to `../06-foundry-iq-grounding-with-ai-search` for RAG with AI Search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nas6nxnzbl",
   "source": "---\n\n<div align=\"center\">\n\n## License & Attribution\n\nThis notebook is part of the **Azure AI Foundry Demo Repository**\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](../LICENSE)\n\n**Original Author:** Ozgur Guler | AI Solution Leader, AI Innovation Hub\n\n**Contact:** [ozgur.guler1@gmail.com](mailto:ozgur.guler1@gmail.com)\n\n---\n\n*If you use, modify, or distribute this work, you must provide appropriate credit to the original author as required by the [Apache License 2.0](../LICENSE).*\n\n**Copyright © 2025 Ozgur Guler. All rights reserved.**\n\n</div>",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}